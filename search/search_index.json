{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"devops-philosophy/","title":"Devops Philosophy","text":"<p>Devops is a process of improving the application delivery - Automation - Quality  - Continous monitoring - Continous testing</p>"},{"location":"devops-philosophy/#core-principles-of-devops","title":"Core Principles of Devops","text":"<ol> <li>Collaboration - DevOps emphasizes a culture of collaboration and communication between software developers, IT professionals, and business stakeholders.</li> <li>Automation - Automate testing, deployment, and infrastructure provisioning.</li> <li>Continuous Ingeration (CI) - CI is the practice of frequently integrating code changes into a shared repository.</li> <li>Continuous Delivery (CD) - CD extends CI by automatically deploying all code changes to a testing or staging environment after the build stage.</li> <li>Continous Deployment - This is a step beyond continuous delivery. Every change that passes through all stages of the production pipeline is released to customers without manual intervention.</li> <li>Infrastructure as Code (IaC)</li> <li>Monitoring and Logging - Continuous monitoring of the application and infrastructure performance is crucial to understand how system changes affect user experiences.</li> <li>Feedback Loops</li> </ol>"},{"location":"devops-philosophy/#sdlc","title":"SDLC","text":"<ul> <li>Software development lifecycle</li> <li>Design, develop, test, deploy</li> </ul>"},{"location":"devops-philosophy/#philosophy","title":"Philosophy","text":""},{"location":"devops-philosophy/#developers-should-not-need-to-have-any-access-to-aws-eventually","title":"Developers should not need to have any access to AWS eventually","text":"<ul> <li>All monitoring from the hardware level (like CPU, memory and network) to the application level should be exposed through third party tools like Datadog and Logentries.</li> </ul>"},{"location":"devops-philosophy/#no-ssh-into-ec2-instances-ever","title":"No SSH into EC2 instances ever","text":"<ul> <li>The keypair section of AWS EC2 should eventually be empty.</li> <li>Disable public IP on EC2 instances and SSH port 22.</li> </ul>"},{"location":"devops-philosophy/#resource-creation-should-only-be-done-through-deployment-machines","title":"Resource creation should only be done through deployment machines","text":"<ul> <li>Avoid point and click at all cost.</li> <li>Everything should be scripted and the script on Github.</li> <li>All updates to resources should be done via a git push.</li> <li>AWS GUI Console usage is only used to check that the resource has been created.</li> </ul>"},{"location":"devops-philosophy/#do-not-roll-your-own-if-there-is-a-as-a-service-that-does-it","title":"Do not roll your own if there is a *-as-a-service that does it","text":"<ul> <li>Reason: I don't want to manage its availability, updating the software, security, and others.</li> <li>See below - outdated software is dangerous.</li> </ul>"},{"location":"devops-philosophy/#always-use-the-latest-software-version","title":"Always use the latest software version","text":"<ul> <li>Outdated software are dangerous because they might have unresolved security vulnerabilities (remember heartbleed?).</li> <li>Always look for software that can update itself or use a *-as-a-service that manages its updates. For OS, look at CoreOS - the only Linux OS that can update itself.</li> </ul>"},{"location":"devops-philosophy/#prefer-alerts-to-monitoring","title":"Prefer alerts to monitoring","text":"<ul> <li>Instead of spending time looking at graphs of errors or CPU usage, we should create alerts that push the information to us instead of having us pull the information or infer from the graph. This makes troubleshooting faster.</li> </ul>"},{"location":"devops-philosophy/#all-alerts-in-one-place","title":"All alerts in one place","text":"<ul> <li>Instead of having alerts go to individual emails or sms, put all alerts to different channels in Slack. People will subscribe to channels that matter to them and will set notification preferences accordingly.</li> <li>Reason: I do not want to have to manage the email addresses or phone numbers that the different alerts go to.</li> </ul>"},{"location":"devops-philosophy/#immutable-server-pattern","title":"Immutable server pattern","text":"<ul> <li>Martin Fowler - Immutable Server</li> <li>From Netflix blog:<ul> <li>Our deployment strategy is centered around the Immutable Server pattern. Live modification of instances is strongly discouraged in order to reduce configuration drift and ensure deployments are repeatable from source. Every deployment at Netflix begins with the creation of a new Amazon Machine Image, or AMI. To generate AMIs from source, we created \u201cthe Bakery\u201d.</li> <li>We do not use any configuration management systems like Puppet, Chef, Ansible or Saltstack.</li> <li>Devprod parity thus stands here. If you run the same image on your localhost or on AWS, you'll get the same result.</li> </ul> </li> </ul>"},{"location":"devops-philosophy/#servers-as-cattle-not-pets","title":"Servers as cattle not pets","text":"<ul> <li>Only one kind of operating system, everything should run on Docker on top of it.</li> <li>At the maximum, there should only be 2 kinds of servers: app servers and deployment servers.</li> <li>App servers are servers that run long-running web servers and batch jobs; deployment servers only has aws-cli and does deployments.</li> </ul>"},{"location":"devops-philosophy/#one-vpc-in-one-account","title":"One VPC in one account","text":"<ul> <li>This simplifies a whole lot of things.</li> </ul>"},{"location":"devops-philosophy/#all-apps-should-be-12-factor-apps","title":"All apps should be 12 factor apps","text":"<ul> <li>Print out the twelve factor document and use it as a checklist for all apps - actually tick on the different factors.</li> </ul>"},{"location":"devops-philosophy/#db-access-via-vpn-or-internal-network","title":"DB Access via VPN or internal network","text":"<ul> <li>We don't want our data hacked.</li> </ul>"},{"location":"devops-philosophy/#container-solution-consideration","title":"Container Solution Consideration","text":"<p>These are our considerations when looking for a container solution (example: Kubernetes or ECS):</p> <ol> <li>The ability to automatically scale on the container level. (This can be based on CPU Utilization or similar metric)</li> <li>The ability to automatically scale on the VM level for the cluster. (This can be based on CPU Utilization or similar metric)</li> <li>The ability to automatically deploy using a Continuous Delivery pipeline.</li> <li>The ability to get automatic software update both on the cluster manager level as well as the OS level. Fyi, with Google Container Engine, we get Kubernetes software update automatically with zero down time. And we also get OS updates on the VM level automatically.</li> <li>The ability to automatically manage SSL certificates. Our current Google Container Engine solution is not able to do this so we have to look for 3rd party solution to manage certificate.</li> <li>The ability to manually deploy a particular version, or roll back to a particular version.</li> <li>If we could roll things out to some percentage of our customers just like how Route53 does weighted routing.</li> <li>The ability to manually scale the number of containers to anticipate a spike. One example of this is before a Marketing campaign.</li> </ol>"},{"location":"devops-philosophy/#workflow","title":"Workflow","text":"<p>We adapted GitHub flow so there are always two branches in the project, dev and master, following is an example to show you the flow.</p> <ol> <li>You should have a ticket first, assume the ticker is ESP-123</li> <li>Checkout to dev branch in PWA</li> <li>Create a new branch called feature/ESP-123-some-description</li> <li>Checkout to this new branch, start coding</li> <li>When you finished, create a pull request in GitHub, ask to merge into dev</li> <li>After someone approved your PR, you can merge it and delete the branch</li> </ol> <p>That's all, then you can go to dev env to test it again. You might wonder when will we merge dev to master and deploy it to staging, we do it periodically when dev is stable. You can find details</p> <p>Workflow In sprint planning meeting, we pick up some tickets with PM and include it in the sprint</p> <ol> <li>We move to IN PROGRESS once we are working on this ticket</li> <li>After we finished it and tested on our local environment, we submit a PR in github and move the ticket to IN CODE REVIEW</li> <li>After code review has passed, move to WAITING if it's not ready for QA because it's blocked by others (for example, BE haven't deploy API), otherwise assign the ticket to QA and move to READY FOR QA</li> <li>QA move to IN TESTING</li> <li>QA check this ticket in QA environment and if everything works fine, move to QA PASS. If it's small issue, QA can decide when to deploy and move it to TO PRODUCTION, otherwise will move ticket to PM CHECK and assign to PM.</li> <li>In IN TESTING and PM CHECK, if it fails, will move ticket to REOPENED, just like TODO but indicate that we worked on this ticket before but it fails the test so we need to check again.</li> <li>If PM or QA think it's ready for production, then move the ticket to TO PRODUCTION, and system will automatically assign this ticket to project lead</li> <li>When project lead see this ticket is in the status TO PRODUCTION, will deploy this to production</li> <li>After it's deployed, project lead will move the ticket to DEPLOYED which means it's on production, and system will automatically assign the ticket back to QA</li> <li>QA do a quick check on production again, and move the ticket to CLOSED</li> </ol>"},{"location":"devops-philosophy/#naming-and-tagging","title":"Naming and Tagging","text":""},{"location":"devops-philosophy/#tag","title":"Tag","text":"Key Example project anto4 app xop, bi, de, blog, helmdall env dev, staging, qa, prod group devops, bi, de <p>Reference: AWS Tagging</p>"},{"location":"git/","title":"Git","text":""},{"location":"git/#push-an-existing-repository-to-new-repository","title":"Push an existing repository to new repository","text":"<p>Get current remote repo URL</p> <pre><code>git remote -v\n</code></pre> <p>Set new remote repo</p> <pre><code>git remote set-url origin https://github.com/kimhan9/my-new-repo.git\n</code></pre> <p>Push to repo</p> <pre><code>git branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"git/#git-command","title":"Git command","text":"<p>List which files are staged, unstage and untracked.</p> <pre><code>git status\n</code></pre> <p>Displays committed snapshots.</p> <pre><code>git log\ngit log --oneline\n\n# Display last 3 commits\ngit log -n 3\n</code></pre> <p>Changes on a individual file.</p> <pre><code>git blame &lt;filename&gt;\ngit blame README.md\n</code></pre> <p>Undo a file</p> <pre><code># Undo last change\ngit revert  HEAD\n</code></pre> <p>Reset to a specific commit</p> <pre><code>git reset --hard &lt;ID&gt;\n</code></pre>"},{"location":"git/#cheat-sheet","title":"Cheat Sheet","text":"<p>Gitlab Git Cheat Sheet</p>"},{"location":"git/#git-flow-strategy","title":"Git Flow Strategy","text":"<ul> <li>Have 5 branch type: <code>main</code>, <code>develop</code>, <code>feature</code>, <code>release</code>, <code>hotfix</code></li> <li><code>main</code> branch: contain production-ready code that can be released</li> <li><code>dev</code> branch: contain pre-production code</li> </ul>"},{"location":"gitlab/","title":"Gitlab","text":""},{"location":"gitlab/#gitlab-ci","title":"Gitlab-ci","text":""},{"location":"gitlab/#example-gitlab-ciyaml","title":"Example <code>.gitlab-ci.yaml</code>","text":"<pre><code>stages:\n  - build\n  - test\n  - deploy\n\ndefault:               # Add a default section to define the `image` keyword's default value\n  image: node\n\n.standard-rules:       # Make a hidden job to hold the common rules\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n\nbuild-job:\n  extends:\n    - .standard-rules  # Reuse the configuration in `.standard-rules` here\n  stage: build\n  script:\n    - npm install\n    - npm run build\n  artifacts:\n    paths:\n      - \"build/\"\n\nlint-markdown:\n  stage: test\n  extends:\n    - .standard-rules  # Reuse the configuration in `.standard-rules` here\n  dependencies: []\n  script:\n    - npm install markdownlint-cli2 --global\n    - markdownlint-cli2 -v\n    - markdownlint-cli2 \"blog/**/*.md\" \"docs/**/*.md\"\n  allow_failure: true\n\npages:\n  stage: deploy\n  image: busybox       # Override the default `image` value with `busybox`\n  dependencies:\n    - build-job\n  script:\n    - mv build/ public/\n  artifacts:\n    paths:\n      - \"public/\"\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n</code></pre>"},{"location":"gitlab/#reference","title":"Reference","text":"<ul> <li>Tutorial: Create a complex pipeline</li> <li>Writing .gitlab-ci.yml File with Examples</li> </ul>"},{"location":"gitlab/#using-oidc-openid-connect","title":"Using OIDC - OpenID Connect","text":"<p>OIDC is a way to let developers authenticate services and users without having to manage keys or passwords. An Identity Provider (in this case GitLab) sends a signed JWT read \u2018jot\u2019 (JSON web token) to AWS Security Token Service Api via an <code>AssumeRoleWithWebIdentity</code> call and receives in return a Temporary Security Credential, that allows the GitLab Runner certain actions (depends on the defined AWS Role) on the AWS account.</p>"},{"location":"gitlab/#setup-at-aws","title":"Setup at AWS","text":"<ul> <li> <p>You need to create identity provider in AWS IAM.<sup>1</sup></p> </li> <li> <p>Create role. Change <code>trusted entities</code> to limit authorization to a specific group, project, branch, or tag.</p> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::AWS_ACCOUNT:oidc-provider/gitlab.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"gitlab.com:sub\": \"project_path:kimhan9/*:ref_type:branch:ref:*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Attached policy to that role.</li> </ul>"},{"location":"gitlab/#setup-at-gitlab","title":"Setup at Gitlab","text":"<ul> <li> <p>Set the <code>ROLE_ARN</code> variable created above. (Settings -&gt; CI/CD -&gt; Variables)</p> </li> <li> <p>Sample <code>gitlab-ci.yml</code><sup>2</sup></p> </li> </ul> <pre><code>assume role:\n  image:\n    name: \"amazon/aws-cli:latest\"\n    entrypoint: [\"\"]\n  id_tokens:\n    MY_OIDC_TOKEN:\n      aud: https://gitlab.com\n  variables:\n    AWS_PROFILE: oidc\n    AWS_REGION: ap-southeast-1\n  before_script:\n    - mkdir -p ~/.aws\n    - echo \"${MY_OIDC_TOKEN}\" &gt; /tmp/web_identity_token\n    - echo -e \"[profile oidc]\\nrole_arn=${ROLE_ARN}\\nweb_identity_token_file=/tmp/web_identity_token\" &gt; ~/.aws/config\n  script:\n    - aws sts get-caller-identity\n    - aws s3 ls\n    - aws ec2 describe-instances\n</code></pre>"},{"location":"gitlab/#reference_1","title":"Reference","text":"<ul> <li>Streamline AWS Deployments with GitLab CI and Terraform</li> </ul> <ol> <li> <p>Configure OpenID Connect in AWS to retrieve temporary credentials \u21a9</p> </li> <li> <p>Configure OpenID Connect between GitLab and AWS \u21a9</p> </li> </ol>"},{"location":"ios/","title":"iOS","text":""},{"location":"ios/#fastlane","title":"Fastlane","text":"<pre><code># Install ruby\nrbenv install 2.7.6\n\n# Install fastlane\nxcode-select --install\ngem install fastlane -NV\nsudo xcodebuild -runFirstLaunch\n\n# Insall fastlane and update\nbundle install\nbundle exec fastlane update_plugins\nbundle update fastlane\n\n# Setup cocapods\nsudo gem install cocoapods\npod install\npod update\n\n# Fastlane run test\nbundle exec fastlane tests\n</code></pre>"},{"location":"ios/#to-generate-provision-profile","title":"To generate provision profile","text":"<ul> <li>Update cert/provisioning profile: <code>fastlane match_adhoc</code></li> <li>Register new device: <code>fastlane run register_device udid:\"device-id\" name:\"Kim Hang\"</code></li> <li>Refresh profile with new device <code>fastlane match adhoc --force</code></li> </ul>"},{"location":"linux/","title":"Linux","text":""},{"location":"linux/#linux-file-system-explained","title":"Linux File System Explained","text":""},{"location":"linux/#soft-link-and-hard-link","title":"Soft Link and Hard Link","text":"<ul> <li>Soft links are like shortcuts and can link to files across different filesystems, but they break if the original file is deleted.</li> <li>Hard links create another reference to the same data, remaining valid even if the original file is deleted, but they cannot link to directories or exist across different filesystems.</li> </ul>"},{"location":"linux/#swap-file","title":"Swap File","text":"<ul> <li>Check swap file: <code>free -h</code></li> <li>Show total, free and used swap space: <code>cat /proc/meminfo</code></li> <li>Show active swap device: <code>cat /proc/swaps</code></li> <li>Show swap info: <code>swapon --show</code></li> </ul>"},{"location":"mkdocs/","title":"MkDocs","text":"<p>Material for MkDocs is a powerful documentation framework on top of MkDocs, a static site generator for project documentation.</p>"},{"location":"mkdocs/#installation","title":"Installation","text":"<p>Create a new virtual environment</p> <pre><code>python3 -m venv venv\n</code></pre> <p>Activate the environment</p> <pre><code>source venv/bin/activate\n</code></pre> <p>Install<sup>1</sup><sup>2</sup></p> <pre><code>pip install mkdocs-material\n</code></pre> <p>Bootstrap</p> <pre><code>mkdocs new .\n</code></pre> <p>Adding minimal configuration on <code>mkdocs.yml</code></p> <pre><code>theme:\n  name: material\n</code></pre>"},{"location":"mkdocs/#build-template","title":"Build Template","text":"<pre><code>mkdocs build\n</code></pre>"},{"location":"mkdocs/#preview-as-you-write","title":"Preview as you write","text":"<pre><code>source venv/bin/activate\nmkdocs serve\n</code></pre> <ol> <li> <p>Doc - mkdocs-material \u21a9</p> </li> <li> <p>Youtube - How To Create STUNNING Code Documentation With MkDocs Material Theme \u21a9</p> </li> </ol>"},{"location":"mysql/","title":"MySQL","text":""},{"location":"mysql/#mysql-definition","title":"MySQL Definition","text":"<ul> <li>The Primary Key constraint uniquely identifies each record in a table. Primary keys must contain UNIQUE values, and cannot contain NULL values.</li> <li>The FOREIGN KEY is a field in one table, refers to the PRIMARY KEY of another table.</li> <li><code>KEYs</code> (INDEXes) are defined on certain columns to facilitate fast search on these columns. We would use <code>SHOW INDEX FROM tableName \\G</code> to display the details on indexes.</li> </ul> <pre><code>CREATE INDEX email_index ON Users(email);\n</code></pre>"},{"location":"mysql/#mysql-query","title":"MySQL Query","text":"<ul> <li>w3schools MySQL learning</li> <li>NTU Lecture</li> <li>Sample MySQL DB</li> </ul>"},{"location":"mysql/#mysql-explain","title":"MySQL Explain","text":"<ul> <li>Add <code>EXPLAIN</code> before select statement show analyzing how queries are executed.</li> </ul>"},{"location":"mysql/#mysql-vs-mongodb","title":"MySQL vs MongoDB","text":"MySQL Mongo DB Schema-based Schema-less Relational DB Non-relational DB Table-structure system Document-based system Does not gurantee data integrity due lack of relation"},{"location":"network/","title":"Network","text":""},{"location":"network/#http","title":"HTTP","text":""},{"location":"network/#http-request-method","title":"HTTP Request Method","text":"Method Description <code>GET</code> Used to request data. <code>HEAD</code> Requests the headers that would be returned. <code>POST</code> Send data to the server. <code>PUT</code> Creates a new resource or replaces target resource with the request payload."},{"location":"network/#http-status-code","title":"HTTP Status Code<sup>3</sup>","text":"Method Description <code>2xx</code> Success Code <code>200 OK</code> Success <code>204 No Content</code> The request was successfully processed, but there is no content. The headers may be useful. <code>3xx</code> Redirection code <code>301 Moved Permanently</code> Resource yas moved to new URL. <code>304 Not Modified</code> Used for caching purposes. The response hasn\u2019t been modified. <code>4xx</code> Client error code <code>400 Bad Request</code> Server won't process due to client error. <code>401 Unauthorized</code> The user doesn\u2019t have valid authentication credentials to get the requested resource. <code>403 Forbidden</code> The client doesn\u2019t have access rights to the content. <code>404 Not found</code> Cannot find the resource. <code>405 Method Not Allowed</code> Target resource doesn't support this method. <code>429 Too Many Request</code> Rate limit error <code>5xx</code> Server error code <code>500 Internal Server Error</code> The server has encountered an unexpected error and cannot complete the request. <code>502 Bad Gateway</code> The server acts as a gateway and gets an invalid response from an inbound host. <code>503 Service Unavailable</code> The server is unable to process the request. This often occurs when a server is overloaded or down for maintenance. <code>504 Gateway Timeout</code> he server was acting as a gateway or proxy and timed out, waiting for a response."},{"location":"network/#proxy-vs-reversed-proxy","title":"Proxy vs Reversed Proxy","text":"(Forward) Proxy Reversed Proxy Sit between user and Internet. Forward request on behalf of user Sit between Internet and server. Receive request on behalf of server Avoid browsing restriction. Block certain content. Protect user identify online Load balancing. Protect DDOS attack. Cache static content. Encrpyt and decrypt SSL communication"},{"location":"network/#subnet-mask-calculation","title":"Subnet mask calculation","text":"<ul> <li>eg. For <code>10.0.0.10/20</code>. How many usable hosts? What are the usable range?</li> <li>Full explanation: Calculating the Range of IP Addresses from Subnet Mask</li> <li>IP Subnet Calculator</li> </ul>"},{"location":"network/#osi-7-layers","title":"OSI 7 layers<sup>1</sup>","text":"<ol> <li>Physical Layer<ul> <li>Function: Transmits raw bit streams over a physical medium.</li> <li>Examples: Cables, switches, hubs, and other hardware elements.</li> </ul> </li> <li>Data Link Layer<ul> <li>Function: Provides node-to-node data transfer, error detection and correction, and frame synchronization.</li> <li>Examples: Ethernet, PPP (Point-to-Point Protocol), MAC addresses.</li> </ul> </li> <li>Network Layer<ul> <li>Function: Manages device addressing, tracks the location of devices on the network, and determines the best way to move data.</li> <li>Examples: IP (Internet Protocol), routers.</li> </ul> </li> <li>Transport Layer<ul> <li>Function: Ensures complete data transfer, error recovery, and flow control. It segments and reassembles data for communications between end-to-end nodes.</li> <li>Examples: TCP (Transmission Control Protocol), UDP (User Datagram Protocol).</li> </ul> </li> <li>Session Layer<ul> <li>Function: Manages sessions between applications, establishing, maintaining, and terminating connections.</li> <li>Examples: NetBIOS, RPC (Remote Procedure Call).</li> </ul> </li> <li>Presentation Layer<ul> <li>Function: Translates, encrypts, and compresses data. It ensures that data is in a usable format and is presented to the application layer correctly.</li> <li>Examples: SSL/TLS, JPEG, GIF.</li> </ul> </li> <li>Application Layer<ul> <li>Function: Provides network services directly to applications. It interfaces with software applications and provides services such as email, file transfer, and web browsing.</li> <li>Examples: HTTP, FTP, SMTP.</li> </ul> </li> <li>Mnemonics: <code>Please Do Not Throw Sausage Pizza Away</code></li> </ol>"},{"location":"network/#what-happen-when-a-user-connect-to-a-e-commerce-website","title":"What happen when a user connect to a e-commerce website?","text":"<ol> <li>User enter website URL.</li> <li>Browser resolve DNS domain name to retrieve IP address.</li> <li>Browser initialise TCP connection. Establish 3-way handshake.</li> <li>Perform SSL/TLS handshake for HTTPS. Have certification verification, key exchange.</li> <li>Browser send HTTP GET request to fetch homepage.</li> <li>Browser rendering web page. Loading html, javascript, css, images.</li> <li>User interact with the web page. Browsing product, cart which may trigger additional HTTPS request to server and restart process.</li> <li>If there is require user login and authentication. User enter credential and send via HTTPS POST request.</li> <li>Server then validate and establish session for user.</li> </ol>"},{"location":"network/#how-https-works","title":"How HTTPS Works?<sup>2</sup>","text":""},{"location":"network/#tcp-vs-udp","title":"TCP vs UDP","text":"Feature TCP UDP Connection Connection-orientedPerform 3-way handshake Connectionless Reliability ReliableUse acknowledgement, retransmission Unreliableno acknowledgement Header size Bigger Smaller Speed Slower due overhead Faster due minimal overhead Use case Web browsing, email, file transfer Streaming, gaming, broadcast"},{"location":"network/#api-security-best-practise","title":"API Security Best Practise","text":"<ol> <li>Use HTTPS</li> <li>Authentication, use OAuths</li> <li>Rate limit</li> <li>API Versioning</li> <li>Allowlist</li> <li>Check with OWASP API Security Risks</li> <li>Use API Gateway</li> <li>Error handling. Give descriptive helpful message</li> <li>Input validation</li> </ol> <ol> <li> <p>ByteByteGo Blog - Network Protocols Run The Internet \u21a9</p> </li> <li> <p>ByteByteGo Blog - How Does HTTPS Work \u21a9</p> </li> <li> <p>HTTP response status codes \u21a9</p> </li> </ol>"},{"location":"Kubernetes/argocd/","title":"Argo CD","text":""},{"location":"Kubernetes/argocd/#installation","title":"Installation","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"Kubernetes/argocd/#port-forwarding","title":"Port Forwarding","text":"<p>Kubectl port-forwarding can also be used to connect to the API server without exposing the service.</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre>"},{"location":"Kubernetes/argocd/#login","title":"Login","text":"<p>The initial password for the <code>admin</code> account is auto-generated. And stored in secret <code>argocd-initial-admin-secret</code> field <code>password</code>. You can simply retrieve this password using the <code>argocd</code> CLI:</p> <pre><code>argocd admin initial-password -n argocd\n</code></pre> <ol> <li> <p>Doc - ArgoCD \u21a9</p> </li> </ol>"},{"location":"Kubernetes/docker/","title":"Docker","text":""},{"location":"Kubernetes/docker/#key-docker-instructions","title":"Key Docker Instructions","text":"<ol> <li><code>FROM</code>: Set base image</li> <li><code>RUN</code>: Executes a command in a new layer</li> <li><code>COPY</code>: Copy files and directories from build context into container</li> <li><code>ADD</code>: Similar as COPY with added extract feature</li> <li><code>ENV</code>: Set environment variables</li> <li><code>EXPOSE</code>: Inform docker which port to listen</li> <li><code>ENTRYPOINT</code>: Configure container to be executable</li> <li><code>CMD</code>: Specifies a default command when container started</li> <li><code>VOLUME</code>: Create a mount point for external storage volume</li> <li><code>WORKDIR</code>: Set the working directory for subsequent command</li> </ol>"},{"location":"Kubernetes/docker/#best-practices","title":"Best Practices","text":"<ol> <li>Use minial base images</li> <li>Use multi-stage builds</li> <li>Reduce layers. Combining command to reduce number of layer</li> <li>Optimize layer caching. Order least change command at the top</li> <li>Delete log, temporary files and caches to reduce image size</li> <li>Never include sensitive data (password, API key)</li> <li>Use <code>.dockerignore</code></li> <li>Run non-root user</li> <li>Scan for vulnerabilities. Use Trivy</li> </ol>"},{"location":"Kubernetes/docker/#example-dockerfile","title":"Example Dockerfile","text":"<pre><code>FROM nginx:latest\nCOPY ./webapp /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n\n# Example 2\nRUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup\nUSER appuser\n</code></pre>"},{"location":"Kubernetes/docker/#command","title":"Command","text":"<ul> <li><code>docker run</code></li> <li><code>docker build</code></li> <li><code>docker pull</code></li> <li><code>docker push</code></li> <li><code>docker image</code>: List all local docker image</li> <li><code>docker ps</code></li> <li><code>docker stop</code></li> <li><code>docker rm</code>: Remove docker container</li> <li><code>docker rmi</code>: Remove docker image</li> <li><code>docker exec</code></li> <li><code>docker logs</code></li> <li><code>docker compose</code></li> </ul>"},{"location":"Kubernetes/docker/#cmd-vs-entrypoint","title":"CMD vs ENTRYPOINT","text":"<code>entrypoint</code> <code>CMD</code> Set fixed command when container start Set default command/argument Append argument at runtime Can overridden at runtime <pre><code>FROM alpine:latest\nENTRYPOINT [\"ls\"]\nCMD [\"-alh\"]\n</code></pre>"},{"location":"Kubernetes/docker/#add-vs-copy","title":"ADD vs COPY","text":"<code>COPY</code> <code>ADD</code> Copy files and directories to docker image Same as Copy with additional extract compressed files and copy files from remote via URL"},{"location":"Kubernetes/docker/#advantage-of-docker","title":"Advantage of Docker","text":"<ul> <li>Portability: Containers run consistently across different environments</li> <li>Efficiency: Lightweight compared to virtual machines, saving system resources</li> <li>Isolation: Containers are isolated, reducing conflicts between applications</li> <li>Scalability: Easy to scale and distribute containerized applications</li> <li>Rapid Deployment: Quick to build and deploy applications</li> <li>Version Control and Reusability: Facilitates version control and reuse of container images</li> <li>Simplified Configuration: Simplifies the setup and configuration of applications</li> </ul>"},{"location":"Kubernetes/docker/#disadvantage-of-docker","title":"Disadvantage of Docker","text":"<ul> <li>Security Concerns: Containers share the host OS kernel, potentially leading to security vulnerabilities</li> <li>Complexity: Managing and orchestrating numerous containers can be complex</li> <li>Persistent Storage: Managing data persistence for containers can be challenging</li> <li>Performance Overhead: Some performance overhead, especially in high-density environments</li> <li>Compatibility: Not all applications are suitable for containerization</li> <li>Learning Curve: Requires learning new tools and concepts</li> </ul>"},{"location":"Kubernetes/helm/","title":"Helm","text":"<ul> <li>Helm is package manager for Kubernetes.</li> <li>Helm Chart is bundle with one or more Kubernetes manifests.</li> <li>Chart allow to version your manifest files.</li> <li>Helm keep release history of all deployed charts.</li> </ul>"},{"location":"Kubernetes/helm/#basic-commands","title":"Basic commands","text":"<p>Search repo</p> <pre><code>helm search repo &lt;keyword&gt;\n</code></pre> <p>Install</p> <pre><code>helm install &lt;my-name&gt; &lt;chart-name&gt;\n</code></pre> <p>Get manifest of installed charts</p> <pre><code>helm get manifest &lt;my-chart&gt;\n</code></pre> <p>Uninstall</p> <pre><code>helm uninstall &lt;my-chart&gt;\n</code></pre>"},{"location":"Kubernetes/helm/#how-to-create-helm-chart","title":"How to create Helm Chart","text":"<ul> <li>Tutorial: https://helm.sh/docs/chart_template_guide/getting_started/</li> <li>Run <code>helm create &lt;chart-name&gt;</code> to create template.</li> <li>Go <code>templates</code> folder. Some general <code>deployment.yaml</code>, <code>service.yaml</code> has created.</li> </ul>"},{"location":"Kubernetes/helm/#debugging","title":"Debugging","text":"<p>Verify chart if follows best proactices</p> <pre><code>helm lint\n</code></pre> <p>Rendering chart locally</p> <pre><code>helm template --debug\n</code></pre> <p>Dry run install</p> <pre><code>helm install --dry-run --debug &lt;my-chart&gt; ./&lt;chart-dolder&gt;\n</code></pre> <p>Package the chart to distrbute</p> <pre><code>helm package ./hello-world\n</code></pre>"},{"location":"Kubernetes/helm/#kube-prometheus-stack","title":"kube-prometheus-stack","text":"<p>The stack come with</p> <ul> <li>prometheus: Core component. Metric collection engine that collect metrics from the agent.</li> <li>alertmanager: Send alert notification when metrics reach alarm state.</li> <li>grafana: Visualization of metrics.</li> <li>node-exporter: Export node metrics like load average, CPU, memory, storage performance.</li> <li>kube-state-exporter: Export metrics directly from Kubernetes API server. Generate metrics above internal Kubernetes objects such as deployment, service, node, pod.</li> <li>prometheus operator   : Simplify and automate stack setup. Uses Kubernetes Custom Resource Definition (CRD) to manage Prometheus, alertmanager, and related components.</li> </ul>"},{"location":"Kubernetes/kubernetes/","title":"Kubernetes","text":"<ul> <li>Pod accessing another pod in another namespace. Use dns eg <code>curl &lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ul>"},{"location":"Kubernetes/kubernetes/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>Kubernetes Quick Reference</li> <li>Coursera - Kubernetes Cheat Sheet</li> <li>A Complete Kubernetes info</li> </ul>"},{"location":"Kubernetes/kubernetes/#basic-yaml","title":"Basic yaml","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/kubernetes/#kubernetes-controller","title":"Kubernetes Controller","text":""},{"location":"Kubernetes/kubernetes/#replicasets","title":"ReplicaSets","text":"<ul> <li>Make sure number of replica pod running</li> </ul>"},{"location":"Kubernetes/kubernetes/#statefulset","title":"StatefulSet","text":"<ul> <li>Manage stateful applications with persistent storage and stable identities</li> <li>Applications like databases (e.g., MySQL, Cassandra) that need persistent storage</li> </ul>"},{"location":"Kubernetes/kubernetes/#deployment","title":"Deployment","text":"<ul> <li>Extension of replicaset which additional feature like rolling update, rollback, versioning</li> </ul>"},{"location":"Kubernetes/kubernetes/#daemonset","title":"Daemonset","text":"<ul> <li>Ensure a copy of pod runs on every node within cluster.</li> <li>Use case:<ul> <li>Log agent to collect log</li> <li>Monitoring agent to monitor node</li> <li>Run specific network plugins</li> <li>Security agent to maintain integrity and security of cluster</li> </ul> </li> </ul>"},{"location":"Kubernetes/kubernetes/#kubernetes-component","title":"Kubernetes Component","text":""},{"location":"Kubernetes/kubernetes/#master-node","title":"Master Node","text":"<ul> <li>API Server: Gateway to control panel, handling request</li> <li>Scheduler: Assign work, like pod to node</li> <li>Controller Manager: Manage controller that regulate state of cluster</li> <li>etcd: Key-value store for cluster data</li> </ul>"},{"location":"Kubernetes/kubernetes/#worker-node","title":"Worker Node","text":"<ul> <li>Kubelet: Agent that ensure container are run in pod</li> <li>Kube-proxy: Manage network communication between pod and external traffic</li> <li>Container runtime: Software that run container, eg docker</li> </ul>"},{"location":"Kubernetes/kubernetes/#terms","title":"Terms","text":"<ul> <li>Node affinity: Control which node your pod can schedule on</li> </ul>"},{"location":"Kubernetes/kubernetes/#pv-and-pvc","title":"PV and PVC","text":"<ul> <li>PV provide way to store data in cluster.</li> <li>PVC request specfic resource from PV.</li> </ul>"},{"location":"Kubernetes/kubernetes/#concept-of-ingress","title":"Concept of Ingress","text":"<ul> <li>Ingress is an API object that manage external access to services in cluster, typical HTTP. Ingress can provide laod balancing, SSL termination and name-based virtual hosting.</li> </ul>"},{"location":"Kubernetes/kubernetes/#how-to-secure-kubernetes","title":"How to Secure Kubernetes","text":"<ul> <li>Network policy</li> <li>Role-based access control (RBAC)</li> <li>Secret manager</li> <li>Use namspaces</li> <li>Secure API servers</li> <li>Ensure the cluster is up to date and security patches</li> </ul>"},{"location":"Kubernetes/minikube/","title":"Minikube","text":"<ul> <li>Installation: <code>brew install minikube</code></li> <li>Start cluster: <code>minikube start</code></li> <li>Check status: <code>minikube status</code></li> </ul>"},{"location":"Kubernetes/minikube/#method-1-install-prometheus-grafana","title":"Method 1: Install Prometheus &amp; Grafana","text":"<ul> <li> <p>Create namespace kubectl create namespace monitoring</p> </li> <li> <p>Deploy prometheus-config.yaml kubectl apply -f https://gist.githubusercontent.com/kimhan9/39227155606edc9830a6274a986bafa6/raw/8c0a821408a76ae0d767fd244ef262ca600f7776/prometheus-config.yaml -n monitoring</p> </li> <li> <p>Deploy prometheus-deployment.yaml kubectl apply -f https://gist.githubusercontent.com/kimhan9/350fafed9ed9511e38675e56d4c712b4/raw/e52d1651a9faa147fa03607e1650c69830b0c15a/prometheus-deployment.yaml -n monitoring</p> </li> <li> <p>Deploy prometheus-service.yaml kubectl apply -f https://gist.githubusercontent.com/kimhan9/15f9f79bf626d60163a0cfdc1eef7848/raw/b5f61ae424a51c166feed4a935e1c089718e485b/prometheus-service.yaml -n monitoring</p> </li> <li> <p>Install grafana helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm install grafana grafana/grafana --namespace monitoring kubectl get pods -n monitoring</p> </li> <li> <p>Access Grafana kubectl get service -n monitoring kubectl expose service grafana --type=NodePort --target-port=3000 --name=grafana-ext -n monitoring minikube service grafana-ext Import Grafana ID: 315</p> </li> </ul>"},{"location":"Kubernetes/minikube/#method-2-install-prometheus-grafana","title":"Method 2: Install Prometheus &amp; Grafana","text":"<ul> <li> <p>Deploy Google Demo Services <code>kubectl apply -f config-microservices.yaml</code></p> </li> <li> <p>Set up Helm. Add Prometheus community repo</p> </li> </ul> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre> <ul> <li>Install Prometheus and Grafana</li> </ul> <pre><code>kubectl create namespace monitoring\nhelm install monitoring prometheus-community/kube-prometheus-stack -n monitoring\nkubectl get pods -n monitoring\nkubectl get svc -n monitoring\n</code></pre> <ul> <li>Access Prometheus and Grafana</li> </ul> <pre><code>kubectl port-forward service/monitoring-kube-prometheus-prometheus -n monitoring 9090:9090 &amp;\nkubectl port-forward service/monitoring-grafana 8080:80 -n monitoring &amp;\n</code></pre> <ul> <li>Grafana login</li> </ul> <pre><code>Username: admin\nPassword: prom-operator\n</code></pre>"},{"location":"app/mitre-caldera/","title":"MITRE Caldera","text":"<p>MITRE Caldera is a cyber security platform designed to easily automate adversary emulation, assist manual red-teams, and automate incident response.<sup>1</sup></p>"},{"location":"app/mitre-caldera/#installation-on-x86-machine","title":"Installation on x86 machine<sup>2</sup>","text":"<pre><code>sudo apt install python3-pip python3-venv python3-dev\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\n\nsudo apt install npm\npython3 server.py --insecure --build\n\n# Golang 1.22 install\necho \"Installing Golang 1.22\"\nsudo wget https://go.dev/dl/go1.22.0.linux-amd64.tar.gz\nsudo tar -C /usr/local/ -xvf go1.22.0.linux-amd64.tar.gz  \necho \"export GOROOT=/usr/local/go\" &gt;&gt; /home/ubuntu/.profile\necho \"export GOPATH=$HOME/go\" &gt;&gt; /home/ubuntu/.profile \necho \"export PATH=$PATH:/usr/local/go/bin\" &gt;&gt; /home/ubuntu/.profile\n\nsudo add-apt-repository ppa:longsleep/golang-backports\nsudo apt update\nsudo apt install golang-go\n\nsudo apt-get install -y apt-transport-https ca-certificates gnupg2 \nsudo apt install software-properties-common -y\nsudo add-apt-repository ppa:deadsnakes/ppa --yes\nsudo apt install upx -y\nsudo apt install python3.9 -y\nsudo apt install python3-pip -y\nsudo apt-get install haproxy -y\n# Upgrade pyOpenSSL - weird issue only impacting AWS EC2 AMI images\npip3 install --upgrade pyOpenSSL\n\n# Install NodeJS for Caldera 5.0 requirement\ncurl -fsSL https://deb.nodesource.com/setup_21.x | sudo -E bash - &amp;&amp;\\\nsudo apt-get install -y nodejs\n\n# Installing docker needed by VECTR\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update -y\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre>"},{"location":"app/mitre-caldera/#docker-deployment","title":"Docker deployment","text":"<pre><code># Install required tool\nsudo apt install git docker.io -y\nsudo usermod -aG docker $(whoami)\n# Relogin the system\n\n# Make docker image\ngit clone https://github.com/mitre/caldera.git --recursive\ncd caldera\ndocker build --build-arg WIN_BUILD=true . -t caldera:latest\n\n# Run docker image\ndocker run -p 8888:8888 caldera:latest\ndocker run -p 7010:7010 -p 7011:7011/udp -p 7012:7012 -p 8888:8888 caldera:latest\n\n# Edit if use public IP\ncd plugins/magma\n# vi .env and enter following line\nVITE_CALDERA_URL=http://&lt;server IP&gt;:8888\n\n# Get login password\ndocker exec -it naughty_wu cat conf/local.yml\n</code></pre> <ol> <li> <p>Calera homepage \u21a9</p> </li> <li> <p>Bootstrap \u21a9</p> </li> </ol>"},{"location":"app/rclone/","title":"Rclone","text":"<p>Rclone syncs files to cloud storage</p> <pre><code>rclone -v --config=rclone.conf --drive-shared-with-me copyto gdrive:'Master Relive Admin File V1.1.xlsx' master.xlsx\nssconvert -O \"separator=, sheet='Restaurant settings'\" master.xlsx master2.txt\n</code></pre>"},{"location":"app/twingate/","title":"Twingate","text":""},{"location":"app/twingate/#update-twingate","title":"Update Twingate","text":"<pre><code>ssh -i ~/.ssh/mykey.pem ubuntu@&lt;server-ip&gt;\ncurl -s &lt;https://binaries.twingate.com/connector/docker-upgrade.sh&gt; | nohup sudo bash\n</code></pre>"},{"location":"cicd/ansible/","title":"Ansible","text":""},{"location":"cicd/ansible/#terms","title":"Terms","text":"<ul> <li> <p>Ansible works by connecting to the remote machine by SSH. Then it execute the tasks defined in the playbook, which are YAML files containing list of tasks to be performed</p> </li> <li> <p>Control Node: A control node is a system where Ansible is installed and set up to connect to your server.</p> </li> <li>Managed Nodes: The systems you control using Ansible are called managed nodes. Ansible requires that managed nodes are reachable via SSH, and have Python 2 (version 2.6 or higher) or Python 3 (version 3.5 or higher) installed.</li> <li>Group: Several hosts group together.</li> <li>Inventory: An inventory file contains a list of the hosts you\u2019ll manage using Ansible.</li> <li>Modules: Unit of code that Ansible send to the remote nodes for execution, such as installing software, managing files, or executing commands.</li> <li>Tasks: A task is an individual unit of work to execute on a managed node. Each action to perform is defined as a task. Tasks can be executed as a one-off action via ad-hoc commands, or included in a playbook as part of an automation script.</li> <li>Playbooks: A playbook contains an ordered list of tasks, and a few other directives to indicate which hosts are the target of that automation, whether or not to use a privilege escalation system to run those tasks, and optional sections to define variables or include files.</li> <li>Handlers: Handlers are used to perform actions on a service, such as restarting or stopping a service that is actively running on the managed node\u2019s system. Handlers are typically triggered by tasks, and their execution happens at the end of a play, after all tasks are finished. </li> <li>Roles: A role is a set of playbooks and related files organized into a predefined structure that is known by Ansible. Roles facilitate reusing and repurposing playbooks into shareable packages of granular automation for specific goals, such as installing a web server, installing a PHP environment, or setting up a MySQL server.</li> </ul>"},{"location":"cicd/ansible/#setup-test-environment-with-digitalocean","title":"Setup test environment with DigitalOcean","text":"<ul> <li>Use local machine as control node and DigitalOcean droplets as remote hosts.</li> <li>Refer DigitalOcean to provision droplets.</li> </ul>"},{"location":"cicd/ansible/#command","title":"Command","text":""},{"location":"cicd/ansible/#adhoc-command","title":"Adhoc command","text":"<pre><code># Test connection to the ansible hosts\nansible all -i hosts -m ping\nansible app -i hosts -m ping\n\n# Format\nansible target -i inventory -mmodule -a \"module options\"\n\n# Run bash\nansible all -i hosts -a \"uptime\"\nansible server1 -i inventory -a \"tail /var/log/nginx/error.log\" --become \n\n# Install and removing package\nansible all -i hosts -m apt -a \"name=nginx\" --become\nansible all -i hosts -m apt -a \"name=nginx state=absent\" --become\n\n# Restart service\nansible webservers -i inventory -m service -a \"name=nginx state=restarted\" --become -K\n\n# Copy file\nansible all -i hosts -m copy -a \"src=./fileA dest=~/fileA\"\n\n# Gather info about host\nansible host1 -i hosts -m setup -a \"filter=*ipv*\"\n</code></pre>"},{"location":"cicd/ansible/#playbook-command","title":"Playbook command","text":"<pre><code># Executing playbook\nansible-playbook -i hosts playbook.yml -vvv\n\n# Listing playbook task\nansible-playbook -i hosts playbook.yml --list-tasks\n\n# List/Executing/Skipping by tags\nansible-playbook -i hosts playbook.yml --list-tags\nansible-playbook -i hosts playbook.yml --tags=setup\nansible-playbook -i hosts playbook.yml --exclude-tags=setup\n\n# Starting executing at specific task\nansible-playbook -i hosts playbook.yml --start-at-task=Copy index page\n\n# Limit targets\nansible-playbook -l dev -i hosts playbook.yml\n</code></pre>"},{"location":"cicd/ansible/#adhoc-command_1","title":"Adhoc Command","text":"<pre><code># Copy without hosts file\nansible all -i 'node_ip,' -m copy -a \"src=source_directory dest=destination_directory\"\n\n# To excute with root user\nansible bothservers -m yum -a \"name=tree state=present\" --become --become-user root\n\nansible all -i inventory -a \"uptime\"\nansible server1 -i inventory -a \"tail /var/log/nginx/error.log\" --become\n\n# Install packages\nansible all -i inventory -m apt -a \"name=nginx\" --become -K\n\n# Remove packages\nansible all -i inventory -m apt -a \"name=nginx state=absent\" --become -K\n\n# Copy files\nansible all -i inventory -m copy -a \"src=./file.txt dest=~/myfile.txt\"\nansible all -i inventory -m copy -a \"src=~/myfile.txt remote_src=yes dest=./file.txt\"\n\n# Changing file permissions\nansible all -i inventory -m file -a \"dest=/var/www/file.txt mode=600 owner=sammy group=sammy\" --become -K\n\n# Restarting services\nansible webservers -i inventory -m service -a \"name=nginx state=restarted\" --become -K\n</code></pre>"},{"location":"cicd/ansible/#files","title":"Files","text":""},{"location":"cicd/ansible/#sample-hosts-file","title":"Sample <code>hosts</code> file","text":"<pre><code>sudo vi /etc/ansible/hosts\n[app]\nhost1 ansible_host=159.203.163.103 ansible_user=root ansible_port=22 ansible_ssh_private_key_file=/Users/kim/.ssh/digital_ocean\n\n[sample]\nserver1 ansible_host=203.0.113.111\nserver2 ansible_host=203.0.113.112\nserver3 ansible_host=203.0.113.113\n\n[sample:vars]\nansible_user=sammy\nansible_port=22\nansible_ssh_private_key_file=/home/sammy/.ssh/custom_id\n</code></pre>"},{"location":"cicd/ansible/#check","title":"Check","text":"<pre><code># Check inventory\nansible-inventory --list -y\n\n# Check connection\nansible all -m ping -u root\n\n# Adhoc command\nansible all -a \"df -h\" -u root\n</code></pre>"},{"location":"cicd/ansible/#sample-playbookyml","title":"Sample <code>playbook.yml</code>","text":"<p>Playbook to install nginx and vim</p> <pre><code>---\n- hosts: all\n  become: true\n  tasks:\n    - name: Install Packages\n      apt: name={{ item }} update_cache=yes state=latest\n      loop: [ 'nginx', 'vim' ]\n      tags: [ 'setup' ]\n\n    - name: Copy index page\n      copy:\n        src: index.html\n        dest: /var/www/html/index.html\n        owner: www-data\n        group: www-data\n        mode: '0644'\n      tags: [ 'update', 'sync' ]\n</code></pre> <p>Playbook to install nginx, configure and restart</p> <pre><code>---\n- name: Nginx installation and configuration\n  hosts: web_servers\n  become: true\n\n  vars:\n    nginx_conf_path: /etc/nginx/nginx.conf\n    local_nginx_conf: ./nginx.conf\n\n  tasks:\n    - name: Install nginx\n      apt:\n        name: nginx\n        state: latest\n\n    - name: Upload nginx conf\n      copy:\n        src: \"{{ local_nginx_conf }}\"\n        dest: \"{{ nginx_conf_path }}\"\n        owner: root\n        group: root\n        mode: '0644'\n      notify: restart_nginx\n\n    handlers:\n      - name: Restart nginx\n        service:\n          name: nginx\n          state: restarted\n</code></pre> <p>Playbook to install docker</p> <pre><code>---\n- hosts: all\n  become: true\n  vars:\n    container_count: 4\n    default_container_name: docker\n    default_container_image: ubuntu\n    default_container_command: sleep 1d\n\n  tasks:\n    - name: Install aptitude\n      apt:\n        name: aptitude\n        state: latest\n        update_cache: true\n\n    - name: Install required system packages\n      apt:\n        pkg:\n          - apt-transport-https\n          - ca-certificates\n          - curl\n          - software-properties-common\n          - python3-pip\n          - virtualenv\n          - python3-setuptools\n        state: latest\n        update_cache: true\n\n    - name: Add Docker GPG apt Key\n      apt_key:\n        url: https://download.docker.com/linux/ubuntu/gpg\n        state: present\n\n    - name: Add Docker Repository\n      apt_repository:\n        repo: deb https://download.docker.com/linux/ubuntu jammy stable\n        state: present\n\n    - name: Update apt and install docker-ce\n      apt:\n        name: docker-ce\n        state: latest\n        update_cache: true\n\n    - name: Install Docker Module for Python\n      pip:\n        name: docker\n\n    - name: Pull default Docker image\n      community.docker.docker_image:\n        name: \"{{ default_container_image }}\"\n        source: pull\n\n    - name: Create default containers\n      community.docker.docker_container:\n        name: \"{{ default_container_name }}{{ item }}\"\n        image: \"{{ default_container_image }}\"\n        command: \"{{ default_container_command }}\"\n        state: present\n      with_sequence: count={{ container_count }}\n</code></pre> <p>Playbook to initialize server</p> <pre><code>---\n- hosts: all\n  become: true\n  vars:\n    created_username: sammy\n\n  tasks:\n    - name: Install aptitude\n      apt:\n        name: aptitude\n        state: latest\n        update_cache: true\n\n    - name: Setup passwordless sudo\n      lineinfile:\n        path: /etc/sudoers\n        state: present\n        regexp: '^%sudo'\n        line: '%sudo ALL=(ALL) NOPASSWD: ALL'\n        validate: '/usr/sbin/visudo -cf %s'\n\n    - name: Create a new regular user with sudo privileges\n      user:\n        name: \"{{ created_username }}\"\n        state: present\n        groups: sudo\n        append: true\n        create_home: true\n\n    - name: Set authorized key for remote user\n      ansible.posix.authorized_key:\n        user: \"{{ created_username }}\"\n        state: present\n        key: \"{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}\"\n\n    - name: Disable password authentication for root\n      lineinfile:\n        path: /etc/ssh/sshd_config\n        state: present\n        regexp: '^#?PermitRootLogin'\n        line: 'PermitRootLogin prohibit-password'\n\n    - name: Update apt and install required system packages\n      apt:\n        pkg:\n          - curl\n          - vim\n          - git\n          - ufw\n        state: latest\n        update_cache: true\n\n    - name: UFW - Allow SSH connections\n      community.general.ufw:\n        rule: allow\n        name: OpenSSH\n\n    - name: UFW - Enable and deny by default\n      community.general.ufw:\n        state: enabled\n        default: deny\n</code></pre> <p>Playbook to setup Apache and restart server</p> <pre><code>- hosts: all\n  become: true\n\n  tasks:\n    - name: Install Apache\n      apt: name=Apache\n\n  handlers:\n    - name: Reload Apache\n      service:\n        name: apache2\n        state: reloaded\n\n    - name: Restart Apache\n      service:\n        name: apache2\n        state: restarted\n</code></pre>"},{"location":"cicd/ansible/#roles","title":"Roles","text":"<p>Tutorial 1: How to Use Ansible Roles to Abstract your Infrastructure Environment</p> <p>Tutorial 2: Ansible Roles: Basics, Creating &amp; Using</p> <p>Example:</p> <pre><code>- hosts: all\n  become: true\n  roles:\n    - role: webserver\n      vars:\n        nginx_version: 1.17.10-0ubuntu1\n      tags: example_tag\n</code></pre>"},{"location":"cicd/jenkins/","title":"Jenkins","text":""},{"location":"cicd/jenkins/#example","title":"Example","text":"<p>Jenkinsfile to pull from git, build and deploy to kubernetes</p> <pre><code>pipeline {\n    agent any   \n    stages {\n        stage('Git') {\n            steps {\n                git branch: 'main', url: 'https://git-repo-url'\n            }\n        }\n\n        stage('Build image') {\n            steps {\n                script {\n                    docker.build('my-app', './app')\n                    //sh 'docker build -t helloworld .'\n                }\n            }\n        }\n\n        stage('Test') {\n            steps {\n                script {\n                    docker.image('my-app').inside {\n                        sh 'npm install'\n                        sh 'npm test'\n                    }\n                }\n            }\n        }\n\n        stage('Push') {\n            steps {\n                sh 'docker push my-app'\n            }\n        }\n\n        stage('Deploy') {\n            steps {\n                // Update kubernetes manifest\n                // Kubernetes credential\n                sh 'kubectl apply -f helloworld.yaml'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/#key-component-of-jenkinsfile","title":"Key Component of Jenkinsfile","text":"<ul> <li>pipeline: Define the pipeline itself</li> <li>agent: Where the pipeline run</li> <li>stage: Different phase of pipeline, like build, test, deploy</li> <li>step: Task to be perform in each stage</li> </ul>"},{"location":"cicd/jenkins/#why-use-jenkinsfile","title":"Why use Jenkinsfile","text":"<ul> <li>Pipeline as code</li> <li>Consistency and version control</li> <li>Automation</li> <li>Reusability</li> </ul>"},{"location":"cicd/terraform/","title":"Terraform","text":""},{"location":"cicd/terraform/#terraform-vs-ansible","title":"Terraform vs Ansible","text":"Terraform Ansible Focus on infra provisioning and management in declarative way Used for configuration management, applicaiton deployment and automated task using imperative approach Define the desire state and it will create, update and delete resources Define task and step to execute"},{"location":"cicd/terraform/#terms","title":"Terms","text":"<ul> <li>Provider: Plugin that interact with cloud provider</li> <li>Resouces: Basic builing block of Terraform configuration represent single piece of infrastructure</li> <li>Modules: Collection of multiple resources. A way to group reusable code and easier maintain</li> <li>State: A file that track the current state of infrastructure manage by Terraform</li> </ul>"},{"location":"cicd/terraform/#commands","title":"Commands","text":"<pre><code># Basic command\nterraform init\nterraform fmt\nterraform validate\nterraform plan -var-file=my-config -out=plan.out\nterraform apply plan.out\nterraform destroy -auto-approve\n\n# Show state\nterraform state list\nterraform state show 'module.vpc.aws_vpc.this[0]'\n</code></pre>"},{"location":"cicd/terraform/#example","title":"Example","text":"<pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 4.16\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"ap-southeast-1\"\n}\n\nresource \"aws_instance\" \"example_server\" {\n  ami           = \"ami-04e914639d0cca79a\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"Example \"\n  }\n}\n</code></pre>"},{"location":"cicd/terraform/#terraform-on-mac-m1","title":"Terraform on Mac M1","text":"<ul> <li>Use <code>tfenv</code> to handle different terraform version</li> <li>Install: <code>brew install tfenv</code></li> <li>Update: <code>brew update</code></li> <li>Install the version: <code>tfenv install 1.9.8</code></li> <li>Use the version: <code>tfenv use 1.9.8</code></li> <li>List: <code>tfenv list</code></li> </ul>"},{"location":"cloud/digitalocean/","title":"DigitalOcean","text":""},{"location":"cloud/digitalocean/#provision-droplet-using-terraform","title":"Provision droplet using Terraform","text":"<ol> <li>Generate Personal Access Token (API \u2192 Tokens \u2192 Generate New Token)</li> <li>Export the token: <code>export DIGITALOCEAN_ACCESS_TOKEN=\"my-token\"</code></li> <li>Deploy Terraform</li> <li>Get status: <code>terraform show terraform.tfstate</code></li> </ol> <p>Reference: Deploy an application to a DigitalOcean droplet</p>"},{"location":"cloud/digitalocean/#terraform-template","title":"Terraform Template","text":"<p><code>main.tf</code></p> <pre><code>terraform {\n  required_version = \"&gt;= 1.0.0\"\n\n  required_providers {\n    digitalocean = {\n      source  = \"digitalocean/digitalocean\"\n      version = \"~&gt; 2.0\"\n    }\n  }\n}\n\nprovider \"digitalocean\" {}\n\n# The name of the key in DigitalOcean\ndata \"digitalocean_ssh_key\" \"main\" {\n  name = \"mba\"\n}\n\nresource \"digitalocean_droplet\" \"terramino\" {\n  count = 3\n  image     = \"ubuntu-22-04-x64\"\n  name      = \"www\"\n  region    = \"sgp1\"\n  size      = \"s-1vcpu-1gb\"\n  ssh_keys  = [data.digitalocean_ssh_key.main.id]\n}\n\noutput \"ip_address\" {\n  description = \"Droplet IP\"\n  value       = digitalocean_droplet.terramino[*].ipv4_address\n}\n</code></pre>"},{"location":"cloud/AWS/cloudwatch/","title":"CloudWatch","text":""},{"location":"cloud/AWS/cloudwatch/#aws-insight-query","title":"AWS Insight query","text":"<pre><code>fields @timestamp, @message\n| sort @timestamp desc\n| filter @message like 'Access denied'\n\nfields @timestamp, @message\n| sort @timestamp desc\n| filter @message like 'unauthenticated'\n\nfields @timestamp, @message\n| sort @timestamp desc\n| filter @message like /error/\n| limit 20\n\nfields @timestamp, @message\n| sort @timestamp desc\n| filter @message like /FAILED/\n| limit 20\n\n# error\nfields @timestamp, @message\n| filter @message not like /Dump thread metrics/\n| sort @timestamp desc\n\n# audit\nfields @timestamp, @message\n| filter @message like /FAILED_CONNECT/\n| sort @timestamp desc\n\nfields @timestamp, @message\n| sort @timestamp desc\n| filter @message not like /ap-southeast-1a,yanisthesmartest/\n</code></pre>"},{"location":"cloud/AWS/cloudwatch/#manual-trigger-aws-cloudwatch-alarm","title":"Manual trigger AWS CloudWatch Alarm","text":"<pre><code>aws cloudwatch set-alarm-state --alarm-name \"CIS-3.2-ConsoleSigninWithoutMFA\" --state-value ALARM --state-reason \"test\"\ncloudwatch set-alarm-state --alarm-name \"CIS-3.2-ConsoleSigninWithoutMFA\" --state-value OK --state-reason \"test\"\n</code></pre>"},{"location":"cloud/AWS/ecs/","title":"ECS","text":"<p>Generate ECS task definition skeleton</p> <pre><code>aws\u00a0ecs\u00a0register-task-definition\u00a0--generate-cli-skeleton\n</code></pre> <p>Generate ECS service definition skeleton</p> <pre><code>aws ecs create-service --generate-cli-skeleton\n</code></pre>"},{"location":"cloud/AWS/ecs/#cloudformation","title":"Cloudformation","text":"<p>Example ECS Cloudformation template 1</p> <p>Example ECS Cloudformation template 2</p> <pre><code># ECS template\nversion: '3'\nservices:\n  dev:\n    build:\n      context: .\n      dockerfile: cmd/proxy/Dockerfile\n      args:\n        GOLANG_VERSION: 1.13\n      environment:\n        - ATHENS_TRACE_EXPORTER=jaeger\n        - ATHENS_TRACE_EXPORTER_URL=http://172.30.2.27:14268\n        - ATHENS_STORAGE_TYPE=memory\n        - AWS_REGION=ap-southeast-1\n        - AWS_ACCESS_KEY_ID=&lt;access-key&gt;\n        - AWS_SECRET_ACCESS_KEY=&lt;secret&gt;\n        - ATHENS_S3_BUCKET_NAME=kim-athens\n      ports:\n        - 3000:3000\n      depends_on:\n        - jaeger\n    jaeger:\n      environment:\n       - COLLECTOR_ZIPKIN_HTTP_PORT=9441\n      image: jaegertracing/all-in-one:latest\n      ports:\n        - 14268:14268\n        - 9411:9411\n        - 5775:5775/udp\n        - 6831:6831/udp\n        - 6832:6832/udp\n        - 5778:5778\n        - 16686:16686\n\n# Athens parameter\nATHENS_STORAGE_TYPE=s3\nAWS_REGION=ap-southeast-1\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nATHENS_S3_BUCKET_NAME=kim-athens\n\n# Build docker image\ndocker-compose build\n\n# Authenticate your Docker client to your registry.\n$(aws ecr get-login --no-include-email --region ap-southeast-1)\n\n# Push to AWS repository\ndocker-compose push\n\n# Test Athens proxy\nexport PATH=$PATH:/usr/local/go/bin\nexport GO111MODULE=on\nexport GOPROXY=&lt;athens-elb-url&gt;\ngit clone &lt;https://github.com/athens-artifacts/walkthrough.git&gt;\ncd walkthrough\ngo run .\n\n</code></pre>"},{"location":"cloud/AWS/ecs/#create-ecs-service","title":"Create ECS service","text":"<pre><code>aws ecs register-task-definition --cli-input-json file://epps-core-staging-tf.json\n\nSERVICE_NAME=\"epps-core-staging\"\nTARGET_GROUP_ARN=\"&lt;target-group-arn&gt;\"\nCONTAINER_PORT=10082\n\naws ecs create-service \\\n  --cluster Staging \\\n  --service-name ${SERVICE_NAME} \\\n  --task-definition ${SERVICE_NAME} \\\n  --desired-count 1 \\\n  --deployment-configuration \"maximumPercent=200,minimumHealthyPercent=100,deploymentCircuitBreaker={enable=true,rollback=true}\" \\\n  --placement-strategy type=\"spread\",field=\"attribute:ecs.availability-zone\" type=\"binpack\",field=\"memory\" \\\n  --load-balancers \"targetGroupArn=${TARGET_GROUP_ARN},containerName=${SERVICE_NAME},containerPort=${CONTAINER_PORT}\" \\\n  --launch-type EC2\n</code></pre>"},{"location":"cloud/AWS/iam/","title":"IAM","text":""},{"location":"cloud/AWS/iam/#mfa","title":"MFA","text":""},{"location":"cloud/AWS/iam/#enable-mfa-on-aws","title":"Enable MFA on AWS","text":"<ol> <li>You required a virtual multi-factor authentication (MFA) on your phone. eg. Google Authenticator</li> <li>Login to AWS</li> <li>Select you username \u2192 My Security Credentials.</li> <li>Under Muti-factor authentication (MFA), click Assign MFA device.</li> <li>Choose Virtual MFA device \u2192 Continue.</li> <li>Click Show QR code. Use the app to scan the QR code. The app starts generating six-digit numbers. Enter the first six-digit number displayed on app into the MFA code 1. Wait for a while for new number and put it as MFA code 2. Click Assign MFA.</li> <li>User require to attach policy to manage MFA.</li> </ol>"},{"location":"cloud/AWS/iam/#remove-mfa","title":"Remove MFA","text":"<p>MFA need to be deactivate first before remove.</p> <pre><code>aws iam deactivate-mfa-device --user-name &lt;username&gt; --serial-number arn:aws:iam::&lt;account-id&gt;:mfa/&lt;username&gt;\naws iam delete-virtual-mfa-device --serial-number arn:aws:iam::&lt;account-id&gt;:mfa/&lt;username&gt;\n</code></pre>"},{"location":"cloud/AWS/iam/#iam-database-authentication-for-mysql","title":"IAM Database Authentication for MySQL","text":"<p>User connect to Amazon RDS using IAM role.</p>"},{"location":"cloud/AWS/iam/#activate-iam-db-authentication","title":"Activate IAM DB authentication","text":"<p>Enable <code>IAM database authentication</code> by using the Amazon RDS console.</p>"},{"location":"cloud/AWS/iam/#create-a-database-user-account-that-uses-an-aws-authentication-token","title":"Create a database user account that uses an AWS authentication token","text":"<p>Connect to the DB instance or cluster endpoint by with master credential. Run command to create user:</p> <pre><code>CREATE USER {dbusername} IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS';\n</code></pre> <p>By default, the database user is created with no privileges. To require a user account to connect using SSL and other privileges, run command:</p> <pre><code>ALTER USER {dbusername} REQUIRE SSL;\nGRANT SELECT, INSERT, UPDATE, DELETE, ALTER ON *.* TO '{dbusername}'@'%';\n</code></pre>"},{"location":"cloud/AWS/iam/#add-an-iam-policy","title":"Add an IAM policy","text":"<p>Enter a policy that allows the rds-db:connect action to the required user.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       {\n          \"Effect\": \"Allow\",\n          \"Action\": [\n              \"rds-db:connect\"\n          ],\n          \"Resource\": [\n              \"arn:aws:rds-db:ap-southeast-1:111111111111:dbuser:cluster-XXXXXXXXXXXX/*\"\n          ]\n       }\n    ]\n}\n</code></pre>"},{"location":"cloud/AWS/iam/#connect-to-the-rds-db-using-iam-role-credentials","title":"Connect to the RDS DB using IAM role credentials","text":""},{"location":"cloud/AWS/iam/#download-ssl-certificates","title":"Download SSL Certificates","text":"<p>Download the AWS RDS Certificate pem file.</p> <pre><code>wget https://truststore.pki.rds.amazonaws.com/ap-southeast-1/ap-southeast-1-bundle.pem\n</code></pre>"},{"location":"cloud/AWS/iam/#generate-authentication-token-and-connect-to-db","title":"Generate authentication token and connect to DB","text":"<p>You have to generate authentication token first to use to connect to DB. The Authentication tokens have a lifespan of 15 minutes.</p> <pre><code>RDSHOST=\"myrds.ap-southeast-1.rds.amazonaws.com\"\nDBUSERNAME={dbusername}\nTOKEN=\"$(aws rds generate-db-auth-token --hostname $RDSHOST --port 3306 --region ap-southeast-1 --username $DBUSERNAME)\"\nmysql --host=$RDSHOST --port=3306 --ssl-ca=/fullpathtopem/ap-southeast-1-bundle.pem --ssl-mode=VERIFY_CA --enable-cleartext-plugin --user=$DBUSERNAME --password=$TOKEN\n\n</code></pre>"},{"location":"cloud/AWS/iam/#note","title":"Note","text":"<ol> <li>It has tested with mysql cli successfully. For othe mysql client, please read their documentation.</li> <li>The username do not have to be same as AWS account.</li> <li>User still can login using traditional username/password. However another new user need create for login using IAM authentication.</li> <li>User has to login using RDS given endpoint. Alternative DNS name do not work.</li> <li>The Authentication tokens have a lifespan of 15 minutes.</li> <li>In general, consider using IAM database authentication when your applications create fewer than 200 connections per second, and you don't want to manage usernames and passwords directly in your application code.</li> </ol>"},{"location":"cloud/AWS/iam/#references","title":"References","text":"<ul> <li>AWS Docs - IAM database authentication for MariaDB, MySQL, and PostgreSQL</li> <li>AWS re:Post - Amazon RDS with IAM credentialss</li> <li>miztiik - IAM Database Authentication for MySQL</li> </ul>"},{"location":"cloud/AWS/network/","title":"AWS Network","text":""},{"location":"cloud/AWS/network/#security-group-vs-network-access-control-list-nacl","title":"Security Group vs Network Access Control List (NACL)","text":"Security Group NACL Apply to instance Apply to subnet Stateful. Automatically allows return traffic Stateless. Need allow both inbound and outbound Default: deny all inbound Default: allow all traffic Allow rules only Can specify allow and deny rules"},{"location":"cloud/AWS/network/#alb-vs-nlb","title":"ALB vs NLB","text":"Type ALB NLB Layer Operate at layer 7, application layer Operate at layer 4, network layer Protocol Support HTTP, HTTPS protocol Support TCP, UDP, TLS protocol Target type Work with IP, instance, lambda Work with IP, instance, ALB Application-level traffic management and routing Best for high performance, low latency SSL termination, session persistence, content-based routing Media streaming, gaming"},{"location":"cloud/AWS/notes/","title":"Notes","text":""},{"location":"cloud/AWS/notes/#which-aws-service-that-are-not-region-specific","title":"Which AWS service that are not region specific?","text":"<ul> <li>IAM, Route53, Cloudfront</li> </ul>"},{"location":"cloud/AWS/notes/#how-to-make-sure-alb-receive-traffic-from-cloufront","title":"How to make sure ALB receive traffic from Cloufront?","text":"<ul> <li>Configure Cloudfront to add custom header to ALB.</li> <li>Configure ALB only forward request that contain the custom header.</li> </ul>"},{"location":"cloud/AWS/notes/#aws-design-decision","title":"AWS design decision","text":"<ul> <li>Upgrade EOL</li> <li>Use IAC</li> <li>Automated/Continous deployment: blue/green deployment</li> <li>Use AWS auto scaling</li> <li>Allow horizontal scaling</li> <li>Instance swap</li> <li>Self healing, auto recovery</li> <li>Redeploy to multi AZ for resilliency/disaster recovery</li> <li>Enhance data security and protection</li> <li>Use Cloudfront enforce secure end-to-end connection to origin servers by https</li> <li>Use managed services like EC2, RDS</li> <li>Break down to smaller services and contrainerized application for easier management and deployment</li> <li>Opportunistic refactoring. Rewrite application to serverless</li> <li>Standardlise application layers</li> <li>Move to SaaS, cloud-based commercial app</li> </ul>"},{"location":"cloud/AWS/notes/#network","title":"Network","text":"<ul> <li>Build highly availability network connectivity<ul> <li>Use highly available DNS</li> <li>CDN</li> <li>API gateway</li> <li>Load balancing</li> <li>Reverse proxies</li> </ul> </li> <li>AWS ALB and NLB are reverse proxies. Reversed proxy improve web performance by caching, security, evenly distributing traffic</li> <li>Difference between proxy, reversed proxy<ul> <li>(Forward) Proxy sit between user and internet. Forward request on behalf user. For caching, bypass restriction, enhance privacy.</li> <li>Reversed Proxy sit between internet and server. Receive request on behalf server. Use for load balancer, caching, pSSL termination, protecting server from direct exposure</li> </ul> </li> </ul>"},{"location":"cloud/AWS/notes/#aws-security","title":"AWS Security","text":"<ul> <li>AWS Cloudtrail</li> <li>AWS Security Hub<ul> <li>Implement controls from market-proven security standard</li> <li>Aggregate findings from all security services</li> <li>Provide team with a single panel of glass for security findings</li> <li>Enable standard CIS AWS Foundation Benchmarks and AWS Foundational Security Best Practices</li> </ul> </li> <li>AWS Config</li> <li>AWS GuardDuty</li> </ul>"},{"location":"cloud/AWS/notes/#best-practise-managing-cost","title":"Best practise managing cost","text":"<ul> <li>Use AWS Cost Explorer and Budget</li> <li>Implement resource tagging</li> <li>Choose the right model (reserved instance, saving plans)</li> <li>Regulary review and optimize resource usage</li> </ul>"},{"location":"cloud/AWS/notes/#sync-between-2-buckets","title":"Sync between 2 buckets","text":"<pre><code>aws s3 sync s3://&lt;bucket1&gt;/ s3://&lt;bucket2&gt;/ --source-region us-east-1 --region ap-southeast-1\n</code></pre>"},{"location":"cloud/AWS/redis/","title":"Redis","text":"<ul> <li>Connect redis: <code>redis-cli -h &lt;redis-url&gt;.apse1.cache.amazonaws.com</code></li> <li>Default redis port: <code>6379</code></li> <li>Commands:</li> </ul> <pre><code># Check how many connected clients\ninfo clients\n\n# Set, get, delete, check key\nSET name kyle\nGET name\nSET age 26\nGET age\nDEL age\nEXISTS name\n\n# Get all key\nKEY *\n\n# Clear entire cache\nflushall\n\n# Check key time to live\nttl name\n\n# Set key name to have TTL 10s\nexpire name 10\nsetex name 10 kyle\n\n# Set array\nlpush friends john # Add a array\nlrange friends 0-1 # Get array\nlpush friends sally\nrpush friends mike\nlpop friends # take out first item\nrpop friends # take out the last item\n\n# Sets\nSADD hobbies \"weight lifting\"\nSMEMBERS hobbies\nSREM hobbies\n\n# Hash\nHSET person name kyle\nHGET person name\nHGETALL person\nHGET person age 26\nHDEL person age\nHEXISTS person name\nHEXISTS person age\n</code></pre>"},{"location":"programming/bash/","title":"Bash","text":""},{"location":"programming/bash/#check-for-a-particular-file-size","title":"Check for a particular file size.","text":"<p><code>du -sh &lt;filename&gt;</code></p>"},{"location":"programming/bash/#find-listening-port-on-mac","title":"Find listening port on Mac","text":"<pre><code>netstat -an | grep -i listen | grep 80\nnetstat -an | grep LISTEN | grep 80\n</code></pre>"},{"location":"programming/bash/#set","title":"Set","text":"<pre><code>set -x  # debug\nset -e  # exit the script when there is an error\nset -o pipeline  # exit when there is an error in pipe command\n</code></pre>"},{"location":"programming/bash/#awk","title":"AWK","text":"<pre><code># Grep column 1 and 4\necho employee.txt | awk '{print $1 $4}'\n</code></pre>"},{"location":"programming/node/","title":"Node.js","text":""},{"location":"programming/node/#node-on-mac-m1","title":"Node on Mac M1","text":"<p><code>nvm</code> (https://github.com/nvm-sh/nvm) allows install and use different node. </p>"},{"location":"programming/node/#installation","title":"Installation","text":"<pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\n</code></pre>"},{"location":"programming/node/#use-other-node-version","title":"Use other Node version","text":"<pre><code>nvm list\nnvm install v18.20.3\nnvm use v18.20.3\nnvm uninstall v4.1.0\n</code></pre>"},{"location":"programming/python/","title":"Python","text":""},{"location":"programming/python/#virtual-environment","title":"Virtual Environment","text":"<p>To create</p> <pre><code>python3 -m venv venv\n</code></pre> <p>To activate</p> <pre><code>source venv/bin/activate\npip install Flask\n</code></pre> <p>To install the requirements</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"programming/python/#how-to-update-requirements-file","title":"How to update Requirements file","text":"<p>Get into virtual environment</p> <p>Install packages</p> <pre><code>pip install requests\n</code></pre> <p>Generate the requirements file</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"programming/python/#a-python-app-container-example","title":"A Python app container example","text":"<pre><code>FROM python:3.9.6-slim-buster\n\nRUN useradd --create-home appuser\nUSER appuser\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\nCOPY app.py .\nEXPOSE 5000\nCMD [\"python3\", \"./app.py\"]\n</code></pre>"},{"location":"programming/python/#typical-interview-question","title":"Typical interview question","text":"<ol> <li>Different between list and tuple?</li> </ol> <p>List   - Mutable: Element can be change after creation   - Memory usage: Consume more memory   - Performance: Slower iteration compared to tuples but better for insertion and deletion operations</p> <p>Tuple:   - Immutable: Elements cannot be changed after creation   - Memory Usage: Consumes less memory   - Performance: Faster iteration compared to lists but lacks the flexibility of lists</p> <ol> <li> <p>What is init() in Python?</p> </li> <li> <p>Initalize an object state</p> </li> <li> <p>Assign value to object properties</p> </li> <li> <p>Difference between a mutable data type and an immutable data type</p> </li> </ol> <p>Mutable data types:   - Data type can be modified after creation   - Examples: List, Dictionary, Set   - Chracteristics: Element can be added, removed or changed   - Use case: Suitable for collection of item that frequently update needed</p> <p>Immutable data types:   - Data type cannot be modified after creation   - Examples: tuple, string, numeric (int, float)   - Chracteristics: Element cannot be change once set; any operation that appear to modify an immutable object will create a new object</p> <ol> <li>Explain List, Dict and Tuple</li> </ol> <p>List</p> <pre><code>my_list = [1, 2, 3, 4, 5]\n</code></pre> <p>Dictionary</p> <pre><code>my_dict = {1, 2, 3, 4, 5}\n</code></pre> <p>Tuple</p> <pre><code>my_tuple = (1, 2, 3, 4, 5)\n</code></pre>"},{"location":"security/sonarqube/","title":"Sonarqube","text":""},{"location":"security/sonarqube/#setup","title":"Setup","text":"<pre><code>docker run -d --name sonar -p 9000:9000 sonarqube:lts-community\n</code></pre> <p>Sonarqube scanner CLI</p> <pre><code>docker run \\\n    --rm \\\n    -e SONAR_HOST_URL=\"http://${SONARQUBE_URL}\" \\\n    -e SONAR_SCANNER_OPTS=\"-Dsonar.projectKey=${YOUR_PROJECT_KEY}\" \\\n    -e SONAR_TOKEN=\"myAuthenticationToken\" \\\n    -v \"${YOUR_REPO}:/usr/src\" \\\n    sonarsource/sonar-scanner-cli\n</code></pre>"},{"location":"security/sonarqube/#default-login","title":"Default Login","text":"<pre><code>Site: http://localhost:9000\nUsername: admin\nPassword: admin\n</code></pre>"},{"location":"security/sonarqube/#webook-for-jenkins","title":"Webook for jenkins","text":"<pre><code>http://172.17.0.3:8080/sonarqube-webhook/\n</code></pre>"},{"location":"security/trivy/","title":"Trivy","text":"<p>Trivy is a Swiss army knife type of tool for security scanning of various types of artifacts and code. It can scan different targets such as your local filesystem or a container image from a container registry.</p>"},{"location":"security/trivy/#installation","title":"Installation","text":"<pre><code>brew install trivy\n</code></pre>"},{"location":"security/trivy/#test-terraform-module","title":"Test Terraform Module","text":"<pre><code>terraform init\ntrivy config .\ntrivy fs --scanner vuln,misconfig,secret .\n\n# Skipping all files under `examples` folders\ntrivy config . --skip-dirs '**/examples'\n</code></pre>"},{"location":"security/trivy/#scan-terraform-plan","title":"Scan Terraform Plan","text":"<pre><code>terraform plan --out tf.plan\nterraform show -json tf.plan &gt; tfplan.json\ntrivy config tfplan.json\n</code></pre>"},{"location":"security/trivy/#use-trivy-in-ci","title":"Use Trivy in CI","text":"<pre><code># Example\n    - name: Run Trivy vulnerability scanner in fs mode\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        scan-ref: '.'\n        trivy-config: trivy.yaml\n</code></pre>"}]}